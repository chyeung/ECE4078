{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Practical 09: Part 2 - Deep RL</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\";> <b>Let's first make sure that all the required dependencies are installed (you can skip this step if dependencies are already installed)</b></p> \n",
    "\n",
    "### With pip (in your local machine)\n",
    "1. Open Anaconda prompt\n",
    "2. Type ``pip install gym``\n",
    "3. If pip is missing, type ``conda install pip``\n",
    "4. If using a GPU, type ``pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html``\n",
    "5. If using a CPU, type ``pip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html``\n",
    "\n",
    "### In AWS\n",
    "1. Add and execute the following line at the beginning of your notebook\n",
    "```python \n",
    "import sys\n",
    "!{sys.executable} -m pip install gym```\n",
    "2. Kernels with ``torchvision`` and ``pytorch`` are already installed and ready to be used\n",
    "3. Go to ``Kernel -> Change kernel`` and select ``conda_pytorch_latest_p36``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Environment\n",
    "\n",
    "In this notebook, we will use DQN to solve the Cart-Pole environment.\n",
    "\n",
    "<img src=\"Support/images/cartPole.gif\" height=\"400\" width=\"400\" align=\"center\">\n",
    "\n",
    "## State Space\n",
    "The state space of this environment is defined by a 4-tuple where each value represents:\n",
    "\n",
    "<img src=\"Support/images/cartPole_actionSpace.png\" height=\"400\" width=\"400\" align=\"center\">\n",
    "\n",
    "## Action Space\n",
    "There are just two possible discrete actions\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Num</th>\n",
    "<th>Action</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>Push cart to the left</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>Push cart to the right</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## Rewards\n",
    "\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "## Other Considerations\n",
    "- When choosing the starting state, all observations are assigned a uniform random value in [-0.05..0.05]\n",
    "- An episode is considered as terminated if:\n",
    "    - Pole Angle is more than 12 degrees.\n",
    "    - Cart Position is more than 2.4 (center of the cart reaches the edge of the display).\n",
    "    - Episode length is greater than 200.\n",
    "    \n",
    "    \n",
    "**The environment is considered to be solved when the average return is greater than or equal to 195.0 over 100 consecutive trials.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required dependencies\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('Support'))\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Algorithm\n",
    "\n",
    "With a random policy, the cart pole control is very poor. We got only 13 points as return.\n",
    "\n",
    "Let's use Deep Q-learning to improve the performance of the cart pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" /></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = io.open('Support/images/randomPolicy.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation follows the algorithm proposed by *Mnih et .al* in  <a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\">Playing Atari with Deep Reinforcement Learning</a> \n",
    "\n",
    "<img src=\"Support/images/dqn_replay.png\" width=\"600\" height=\"600\" align=\"center\">\n",
    "\n",
    "We have decomposed the algorithm shown above into:\n",
    "- A ReplayMemory class to represent and encode the Replay Buffer\n",
    "- A DQN class to represent our NN function approximator\n",
    "- An agent class which contains the learning logic of the algorithm\n",
    "- A main loop in which transition tuples ('state', 'action', 'next_state', 'reward', 'done') are generated and added to the replay buffer. This loop also calls the agent ``optimize(.)`` method to train our approximator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "Let's first define our representation of the replay buffer. To do so, we will use the class ``ReplayMemory`` shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tuple represents one observation in our environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    A cyclic buffer of bounded size (capacity) that holds the transitions \n",
    "    observed recently. \n",
    "    \n",
    "    It also implements a sample() method for selecting a random \n",
    "    batch of transitions for training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Returns a minibatch of `Transition` randomly\n",
    "        Args:\n",
    "            batch_size (int): Size of mini-bach\n",
    "        Returns:\n",
    "            List[Transition]: Minibatch of `Transition`\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Network\n",
    "\n",
    "Let us now define the Multi Layer Perceptron network that will be used as the function approximator for the action-value function (q-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"DQN Network\n",
    "        Args:\n",
    "        input_dim (int): `state` dimension.\n",
    "        output_dim (int): Number of actions.\n",
    "        hidden_dim (int): Hidden dimension in fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs=4, num_actions=2, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns a Q_value\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, num_inputs)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 2-D tensor of shape (n, num_actions)\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent\n",
    "\n",
    "This class contains the main steps of the Deep Q-learnig algorithm (highlighted in blue) in the image shown above.\n",
    "\n",
    "**TODO**: \n",
    "- Complete the computation of the target value. Use the variables ``non_final_mask`` and ``non_final_next_states`` to do so.\n",
    "- Compute the loss. That is the difference between the target q-values (``expected_q``) and the values estimated by the network (``predicted_q``). Use the attribute ``self.loss_fn(.)``.\n",
    "\n",
    "Keep in mind how the targets are computed in the original algorithm\n",
    "\n",
    "<img src=\"Support/images/targets.png\" width=\"600\" height=\"600\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    \"\"\"DQN Agent\n",
    "    This class contains the main steps of the DQN algorithm\n",
    "    \n",
    "    Attributes:\n",
    "    policy_net (DQN): Function approximator for our target q function\n",
    "    loss_fn (MSELoss): Criterion that measures the mean squared error (squared L2 norm) \n",
    "                       between each element of the predicted and target q-values.\n",
    "    optimizer (Adam): Stochastic gradient optimizer\n",
    "    gamma (float): Discount factor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=123, input_dim=4, output_dim=2, \n",
    "                 hidden_dim=128, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Define instance of DQNAgent\n",
    "        Args:\n",
    "        seed (int): Value used to initialize random number generator\n",
    "        input_dim (int): `state` dimension.\n",
    "        output_dim (int): Number of actions.\n",
    "        hidden_dim (int): Hidden dimension in fully connected layer\n",
    "        \"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        self.policy_net = DQN(input_dim, output_dim, hidden_dim).to(device)\n",
    "                \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "                \n",
    "        self.gamma = torch.tensor(gamma).float().to(device)\n",
    "        \n",
    "    def get_action(self, state, action_space_dim, epsilon):\n",
    "        \"\"\"\n",
    "        Select next action using epsilon-greedy policy\n",
    "        Args:\n",
    "        epsilon (float): Threshold used to decide whether a random or maximum-value action \n",
    "                         should be taken next\n",
    "         Returns:\n",
    "            int: action index\n",
    "        \"\"\"        \n",
    "        with torch.no_grad():\n",
    "            cur_q = self.policy_net(torch.from_numpy(state).float().to(device))\n",
    "        q_value, action = torch.max(cur_q, axis=0)\n",
    "        action = action if torch.rand(1,).item() > epsilon else torch.randint(0, action_space_dim, (1,)).item()\n",
    "        action = torch.tensor([action]).to(device)\n",
    "        return action\n",
    "    \n",
    "    def get_next_q(self, state):\n",
    "        \"\"\"Returns Q_value for maximum valued action at each state s\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, num_inputs)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 1 tensor of shape (n)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            next_q = self.policy_net(state)\n",
    "        q, _ = torch.max(next_q, axis=1)\n",
    "        return q\n",
    "    \n",
    "    def optimize(self, batch):\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        Args:\n",
    "            batch: List[Transition]: Minibatch of `Transition`\n",
    "        Returns:\n",
    "            float: loss value\n",
    "        \"\"\"\n",
    "        \n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "        next_state_batch = torch.stack(batch.next_state)\n",
    "                \n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state is the one after which the simulation ends)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s.item() is not True,\n",
    "                                          batch.done)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.stack([s for i, s in enumerate(batch.next_state)\n",
    "                                            if batch.done[i].item() is not True])\n",
    "\n",
    "        # Compute predicted q-values\n",
    "        predicted_q = self.policy_net(state_batch).gather(1, action_batch).reshape(1,-1)\n",
    "        \n",
    "        # TODO 1: Compute expected values for non-terminal and terminal states (this is our TD target)\n",
    "        target_q = torch.zeros(len(batch.state), device=device)\n",
    "        target_q[non_final_mask] = self.get_next_q(non_final_next_states)\n",
    "        expected_q = reward_batch.reshape(1,-1)+(self.gamma * target_q)\n",
    "\n",
    "        # TODO 2: Compute loss \n",
    "        loss = self.loss_fn(expected_q, predicted_q)\n",
    "        # Use loss to compute gradient and update policy parameters through backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "                \n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function will help us to see the progress of our DQN Agent during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    \"\"\"\n",
    "    Plot evolution of rewards and losses during training\n",
    "    Args:\n",
    "         rewards (list): Cummulative rewards for episodes seen so far\n",
    "         losses (list): Prediction error at each training step\n",
    "    \n",
    "    \"\"\"\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Steps %s.\\nCummulative reward last 10 episodes: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.ylabel(\"Avg. cummulative reward\")\n",
    "    plt.xlabel(\"No. of steps\")\n",
    "    plt.subplot(132)\n",
    "    plt.title('MSE Loss')\n",
    "    plt.ylabel(\"Avg. cummulative reward\")\n",
    "    plt.xlabel(\"No. of steps\")\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define running hyper-parameters and epsilon training sequence\n",
    "memory_capacity = 1000\n",
    "batch_size = 32\n",
    "env_name = \"CartPole-v0\"\n",
    "num_frames = 10000\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 1500\n",
    "gamma = 0.99\n",
    "hidden_dim = 128\n",
    "\n",
    "epsilon_by_step = lambda frame_idx: epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot([epsilon_by_step(i) for i in range(num_frames)])\n",
    "ax.set_xlabel(\"Num. episodes\")\n",
    "ax.set_ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop and Replay Buffer Control\n",
    "\n",
    "This is the main loop of our DQN implementation. Here we generate the samples added to the replay memory and train the agent using a batch sampled for the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define varibles for plotting\n",
    "losses_list, rewards_list, episode_len_list = [], [], []\n",
    "list_epsilon = []\n",
    "\n",
    "# Create instance of reply buffer\n",
    "replay_buffer = ReplayMemory(memory_capacity)\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(env_name)\n",
    "n_actions = env.action_space.n\n",
    "dim_state = env.observation_space.shape[0]\n",
    "\n",
    "# Create agent\n",
    "agent = DQNAgent(seed=1234, input_dim=dim_state, \n",
    "                 output_dim=n_actions, \n",
    "                 hidden_dim=hidden_dim, gamma=gamma)\n",
    "\n",
    "# Reset environment and set all counters and cummulative varibles to zero\n",
    "state, ep_len, losses, episode_reward = env.reset(), 0, 0, 0\n",
    "\n",
    "for frame_idx in range(1, num_frames + 1):    \n",
    "    # Get epsilon\n",
    "    cur_epsilon = epsilon_by_step(frame_idx)\n",
    "    \n",
    "    # Sample action using e-greedy policy\n",
    "    action = agent.get_action(state, n_actions, cur_epsilon)\n",
    "    \n",
    "    # Apply action and observe changes in the environment\n",
    "    next_state, reward, done, _ = env.step(action.item())\n",
    "    episode_reward += reward\n",
    "    \n",
    "    # Transform observation into Transition tuple\n",
    "    t_s = torch.tensor(state).float().to(device)\n",
    "    t_r = torch.tensor([reward]).float().to(device)\n",
    "    t_ns = torch.tensor(next_state).float().to(device)\n",
    "    t_a = action.to(device)\n",
    "    t_done = torch.tensor([done]).bool().to(device)\n",
    "                \n",
    "    # Add new sample to replay buffer\n",
    "    replay_buffer.push(t_s, t_a, t_ns, t_r, t_done)\n",
    "    state = next_state\n",
    "    \n",
    "    ep_len += 1\n",
    "    \n",
    "    # If current episode has finished, reset environment and counters for next episode\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        rewards_list.append(episode_reward)\n",
    "        episode_len_list.append(ep_len)\n",
    "        episode_reward, ep_len = 0, 0\n",
    "    \n",
    "    # If replay buffer has at least batch_size elements, sample batch and train approximator\n",
    "    if len(replay_buffer) > batch_size:\n",
    "        transitions = replay_buffer.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        loss = agent.optimize(batch)\n",
    "        losses_list.append(loss)\n",
    "    \n",
    "    # Every 200 steps we plot the approximator's progress and performance\n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, rewards_list, losses_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now test our trained model\n",
    "\n",
    "Can we do better than the random policy?\n",
    "\n",
    "**Note**: If running in AWS, the visualization will not work. To get the code below running without issue, replace \n",
    "```python\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "```\n",
    "with\n",
    "\n",
    "```python\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True, video_callable=False)\n",
    "\n",
    "```\n",
    "You can verify the performance of your agent by looking at the cummulative reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True, video_callable=False)\n",
    "n_trials = 100\n",
    "list_rewards = []\n",
    "\n",
    "for i in range(n_trials):\n",
    "    observation = env.reset()\n",
    "    ep_return = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(observation, n_actions, 0.01)\n",
    "        observation, reward, done, info = env.step(action.item())\n",
    "        ep_return += reward\n",
    "    list_rewards.append(ep_return)\n",
    "env.close()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(list_rewards)\n",
    "ax.set_xlabel(\"Num. Trial\")\n",
    "ax.set_ylabel(\"Return\")\n",
    "ax.set_title('Average return %s +- %s' % (np.round(np.mean(list_rewards),2), np.round(np.std(list_rewards), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example of the performance obtained after training the agent with the hyper-parameters listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('./Support/images/cartPole_learned.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
